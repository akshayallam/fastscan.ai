{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/cancer/\"\n",
    "sz_vgg=224\n",
    "sz_resnet = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f526b8e894641dc88940eb13272450a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=350), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy   \n",
      "    0      0.748119   0.606453   0.675     \n",
      "    1      0.67873    0.55766    0.675     \n",
      "    2      0.652501   0.594221   0.625     \n",
      "    3      0.632829   0.657175   0.575     \n",
      "    4      0.616546   0.701967   0.55      \n",
      "    5      0.579803   0.711822   0.6       \n",
      "    6      0.559486   0.678439   0.65      \n",
      "    7      0.536117   0.624555   0.575     \n",
      "    8      0.505056   0.642765   0.6       \n",
      "    9      0.470753   0.701627   0.65      \n",
      "    10     0.446673   0.755205   0.625     \n",
      "    11     0.430014   0.778784   0.65      \n",
      "    12     0.405152   0.753731   0.625     \n",
      "    13     0.37613    0.713912   0.65      \n",
      "    14     0.36207    0.699664   0.7       \n",
      "    15     0.34868    0.662404   0.675     \n",
      "    16     0.327923   0.626281   0.675     \n",
      "    17     0.315721   0.665888   0.675     \n",
      "    18     0.296717   0.723239   0.65      \n",
      "    19     0.282196   0.761407   0.65      \n",
      "    20     0.26855    0.770972   0.65      \n",
      "    21     0.261421   0.768816   0.65      \n",
      "    22     0.253211   0.775663   0.7       \n",
      "    23     0.238205   0.772802   0.675     \n",
      "    24     0.233475   0.756062   0.675     \n",
      "    25     0.222562   0.727591   0.675     \n",
      "    26     0.214993   0.744728   0.65      \n",
      "    27     0.201886   0.832991   0.65      \n",
      "    28     0.193535   0.896093   0.625     \n",
      "    29     0.204014   0.927197   0.65      \n",
      "    30     0.204466   0.977436   0.65      \n",
      "    31     0.195862   0.970956   0.675     \n",
      "    32     0.185083   0.884505   0.675     \n",
      "    33     0.176823   0.841816   0.65      \n",
      "    34     0.170955   0.830154   0.675     \n",
      "    35     0.163917   0.803861   0.675     \n",
      "    36     0.159263   0.776617   0.7       \n",
      "    37     0.150753   0.8332     0.7       \n",
      "    38     0.166206   0.85492    0.675     \n",
      "    39     0.156752   0.848242   0.65      \n",
      "    40     0.165878   0.862592   0.675     \n",
      "    41     0.159689   0.867324   0.675     \n",
      "    42     0.150234   0.851843   0.65      \n",
      "    43     0.148136   0.848952   0.625     \n",
      "    44     0.148456   0.845535   0.625     \n",
      "    45     0.140961   0.855359   0.625     \n",
      "    46     0.135      0.868757   0.625     \n",
      "    47     0.131781   0.897715   0.625     \n",
      "    48     0.125168   0.953881   0.625     \n",
      "    49     0.119921   0.984322   0.625     \n",
      "    50     0.114443   1.033035   0.675     \n",
      "    51     0.111532   1.031689   0.7       \n",
      "    52     0.104669   1.035568   0.675     \n",
      "    53     0.108422   1.017174   0.7       \n",
      "    54     0.103309   1.015863   0.7       \n",
      "    55     0.098428   1.052218   0.65      \n",
      "    56     0.102464   1.002439   0.65      \n",
      "    57     0.09737    0.940146   0.65      \n",
      "    58     0.094138   0.916614   0.625     \n",
      "    59     0.091456   0.950323   0.625     \n",
      "    60     0.089931   0.953863   0.65      \n",
      "    61     0.085847   0.975874   0.675     \n",
      "    62     0.082045   1.023517   0.675     \n",
      "    63     0.082511   1.004881   0.675     \n",
      "    64     0.079029   0.982158   0.675     \n",
      "    65     0.077411   0.973531   0.675     \n",
      "    66     0.07702    0.969347   0.675     \n",
      "    67     0.084089   0.97466    0.675     \n",
      "    68     0.082074   1.000472   0.65      \n",
      "    69     0.080045   1.020647   0.675     \n",
      "    70     0.083589   1.055258   0.7       \n",
      "    71     0.079968   1.064132   0.7       \n",
      "    72     0.075977   1.080006   0.7       \n",
      "    73     0.076403   1.084627   0.7       \n",
      "    74     0.072732   1.063873   0.7       \n",
      "    75     0.070636   1.102088   0.65      \n",
      "    76     0.069061   1.058771   0.65      \n",
      "    77     0.072376   1.035469   0.65      \n",
      "    78     0.069443   1.07398    0.65      \n",
      "    79     0.065682   1.111831   0.675     \n",
      "    80     0.063483   1.135217   0.675     \n",
      "    81     0.07384    1.130552   0.65      \n",
      "    82     0.071651   1.08118    0.625     \n",
      "    83     0.0732     1.018623   0.625     \n",
      "    84     0.068528   0.990496   0.65      \n",
      "    85     0.066725   1.013274   0.675     \n",
      "    86     0.065633   1.04368    0.65      \n",
      "    87     0.064033   1.076634   0.65      \n",
      "    88     0.060491   1.139503   0.65      \n",
      "    89     0.058002   1.152975   0.65      \n",
      "    90     0.056689   1.169122   0.65      \n",
      "    91     0.053708   1.166284   0.675     \n",
      "    92     0.060584   1.190261   0.65      \n",
      "    93     0.057787   1.108381   0.65      \n",
      "    94     0.060888   1.083025   0.65      \n",
      "    95     0.057217   1.134774   0.625     \n",
      "    96     0.057388   1.155761   0.625     \n",
      "    97     0.053633   1.157678   0.625     \n",
      "    98     0.050389   1.118975   0.65      \n",
      "    99     0.048688   1.138702   0.65      \n",
      "   100     0.046483   1.151432   0.65      \n",
      "   101     0.044359   1.172149   0.65      \n",
      "   102     0.042941   1.181178   0.65      \n",
      "   103     0.041582   1.176446   0.65      \n",
      "   104     0.042421   1.145612   0.65      \n",
      "   105     0.04055    1.03788    0.625     \n",
      "   106     0.039328   0.997214   0.625     \n",
      "   107     0.03765    0.998848   0.625     \n",
      "   108     0.039289   1.007206   0.65      \n",
      "   109     0.042523   1.046898   0.65      \n",
      "   110     0.046713   1.076278   0.6       \n",
      "   111     0.044523   1.07107    0.625     \n",
      "   112     0.053597   1.08826    0.6       \n",
      "   113     0.06582    1.064812   0.65      \n",
      "   114     0.061648   1.096727   0.675     \n",
      "   115     0.058429   1.136011   0.675     \n",
      "   116     0.059538   1.107742   0.675     \n",
      "   117     0.058865   1.080718   0.675     \n",
      "   118     0.057088   1.101634   0.675     \n",
      "   119     0.053456   1.126106   0.65      \n",
      "   120     0.050642   1.146745   0.65      \n",
      "   121     0.049179   1.176763   0.65      \n",
      "   122     0.071177   1.121755   0.65      \n",
      "   123     0.066254   1.001965   0.625     \n",
      "   124     0.063624   0.944431   0.65      \n",
      "   125     0.075373   0.945934   0.65      \n",
      "   126     0.075212   0.933775   0.65      \n",
      "   127     0.072378   0.94979    0.625     \n",
      "   128     0.068094   0.988735   0.625     \n",
      "   129     0.067473   1.014629   0.625     \n",
      "   130     0.063819   1.041369   0.625     \n",
      "   131     0.0592     1.065622   0.65      \n",
      "   132     0.056232   1.102329   0.625     \n",
      "   133     0.052765   1.173391   0.65      \n",
      "   134     0.049616   1.184105   0.625     \n",
      "   135     0.047164   1.175694   0.625     \n",
      "   136     0.04477    1.15951    0.625     \n",
      "   137     0.042388   1.167793   0.625     \n",
      "   138     0.040429   1.184684   0.65      \n",
      "   139     0.038437   1.177903   0.625     \n",
      "   140     0.03592    1.163933   0.625     \n",
      "   141     0.035436   1.15669    0.625     \n",
      "   142     0.036953   1.183335   0.625     \n",
      "   143     0.037066   1.184672   0.625     \n",
      "   144     0.041738   1.170859   0.625     \n",
      "   145     0.044399   1.104347   0.625     \n",
      "   146     0.054746   1.077219   0.625     \n",
      "   147     0.064424   1.077801   0.575     \n",
      "   148     0.06538    1.158858   0.6       \n",
      "   149     0.061021   1.135212   0.6       \n",
      "   150     0.057328   1.118577   0.575     \n",
      "   151     0.056434   1.137261   0.575     \n",
      "   152     0.053095   1.217198   0.6       \n",
      "   153     0.051062   1.262254   0.6       \n",
      "   154     0.052751   1.248073   0.625     \n",
      "   155     0.048688   1.263676   0.625     \n",
      "   156     0.047738   1.312525   0.65      \n",
      "   157     0.044892   1.343026   0.65      \n",
      "   158     0.045965   1.367787   0.675     \n",
      "   159     0.043207   1.278256   0.675     \n",
      "   160     0.041158   1.207048   0.65      \n",
      "   161     0.051517   1.141196   0.65      \n",
      "   162     0.047864   1.044307   0.65      \n",
      "   163     0.047455   1.045269   0.65      \n",
      "   164     0.044042   1.027516   0.65      \n",
      "   165     0.062775   1.02712    0.625     \n",
      "   166     0.089588   0.992833   0.65      \n",
      "   167     0.084241   1.013302   0.625     \n",
      "   168     0.08046    1.061106   0.625     \n",
      "   169     0.077421   1.102365   0.6       \n",
      "   170     0.074907   1.159251   0.625     \n",
      "   171     0.072989   1.102884   0.65      \n",
      "   172     0.094769   1.026412   0.675     \n",
      "   173     0.088945   0.954915   0.7       \n",
      "   174     0.083855   0.92516    0.7       \n",
      "   175     0.08998    0.920057   0.7       \n",
      "   176     0.090348   0.97012    0.675     \n",
      "   177     0.090896   0.985569   0.675     \n",
      "   178     0.083923   1.063751   0.675     \n",
      "   179     0.077748   1.118189   0.65      \n",
      "   180     0.072146   1.13428    0.65      \n",
      "   181     0.067425   1.140444   0.65      \n",
      "   182     0.067416   1.181503   0.675     \n",
      "   183     0.062931   1.088767   0.65      \n",
      "   184     0.058735   1.083319   0.675     \n",
      "   185     0.055425   1.084734   0.65      \n",
      "   186     0.051593   1.083852   0.625     \n",
      "   187     0.048248   1.118349   0.65      \n",
      "   188     0.048267   1.156334   0.65      \n",
      "   189     0.045699   1.183754   0.675     \n",
      "   190     0.049978   1.178871   0.65      \n",
      "   191     0.046773   1.150975   0.625     \n",
      "   192     0.044701   1.072302   0.625     \n",
      "   193     0.042344   1.072208   0.65      \n",
      "   194     0.039714   1.078467   0.625     \n",
      "   195     0.043866   1.087457   0.625     \n",
      "   196     0.041965   1.20269    0.625     \n",
      "   197     0.039473   1.239007   0.6       \n",
      "   198     0.053289   1.290601   0.65      \n",
      "   199     0.066084   1.254775   0.65      \n",
      "   200     0.063722   1.27953    0.65      \n",
      "   201     0.059658   1.267141   0.675     \n",
      "   202     0.059646   1.285826   0.675     \n",
      "   203     0.056505   1.269794   0.65      \n",
      "   204     0.055251   1.276037   0.65      \n",
      "   205     0.052642   1.310366   0.625     \n",
      "   206     0.052202   1.337205   0.6       \n",
      "   207     0.048847   1.310159   0.65      \n",
      "   208     0.047544   1.346473   0.6       \n",
      "   209     0.046062   1.344545   0.6       \n",
      "   210     0.047591   1.314611   0.6       \n",
      "   211     0.047529   1.262706   0.65      \n",
      "   212     0.076134   1.253406   0.65      \n",
      "   213     0.069687   1.262148   0.625     \n",
      "   214     0.064441   1.257755   0.6       \n",
      "   215     0.062819   1.231586   0.625     \n",
      "   216     0.061052   1.215571   0.625     \n",
      "   217     0.056164   1.184604   0.625     \n",
      "   218     0.052063   1.181512   0.65      \n",
      "   219     0.048907   1.196137   0.65      \n",
      "   220     0.045964   1.188778   0.65      \n",
      "   221     0.042945   1.219527   0.65      \n",
      "   222     0.040325   1.195138   0.65      \n",
      "   223     0.038179   1.19696    0.65      \n",
      "   224     0.046684   1.205638   0.65      \n",
      "   225     0.050234   1.165842   0.625     \n",
      "   226     0.046403   1.192608   0.625     \n",
      "   227     0.046226   1.212918   0.625     \n",
      "   228     0.043655   1.219753   0.6       \n",
      "   229     0.04118    1.232228   0.6       \n",
      "   230     0.038383   1.236196   0.625     \n",
      "   231     0.037693   1.231998   0.6       \n",
      "   232     0.043831   1.241544   0.625     \n",
      "   233     0.041376   1.139332   0.6       \n",
      "   234     0.044105   1.14607    0.6       \n",
      "   235     0.042751   1.13969    0.6       \n",
      "   236     0.04065    1.138255   0.625     \n",
      "   237     0.040122   1.105546   0.65      \n",
      "   238     0.040742   1.163317   0.625     \n",
      "   239     0.038537   1.249666   0.65      \n",
      "   240     0.038177   1.275029   0.675     \n",
      "   241     0.03543    1.366752   0.7       \n",
      "   242     0.034778   1.418563   0.675     \n",
      "   243     0.035097   1.403404   0.7       \n",
      "   244     0.032969   1.386999   0.7       \n",
      "   245     0.032402   1.371845   0.7       \n",
      "   246     0.053059   1.344019   0.675     \n",
      "   247     0.048642   1.30252    0.675     \n",
      "   248     0.052224   1.27047    0.675     \n",
      "   249     0.048168   1.247817   0.675     \n",
      "   250     0.047718   1.265591   0.65      \n",
      "   251     0.043984   1.273586   0.675     \n",
      "   252     0.040531   1.300921   0.65      \n",
      "   253     0.038367   1.299503   0.675     \n",
      "   254     0.036743   1.322218   0.675     \n",
      "   255     0.034073   1.33035    0.675     \n",
      "   256     0.034888   1.387337   0.625     \n",
      "   257     0.033826   1.438867   0.7       \n",
      "   258     0.032232   1.433929   0.7       \n",
      "   259     0.030126   1.452436   0.7       \n",
      "   260     0.028522   1.473432   0.7       \n",
      "   261     0.026347   1.449084   0.7       \n",
      "   262     0.027671   1.469833   0.65      \n",
      "   263     0.037525   1.45636    0.65      \n",
      "   264     0.070056   1.526329   0.675     \n",
      "   265     0.080476   1.622503   0.65      \n",
      "   266     0.076196   1.586886   0.6       \n",
      "   267     0.072468   1.526349   0.6       \n",
      "   268     0.067497   1.439623   0.625     \n",
      "   269     0.06399    1.368277   0.625     \n",
      "   270     0.060129   1.301151   0.625     \n",
      "   271     0.055257   1.241989   0.65      \n",
      "   272     0.052698   1.216852   0.65      \n",
      "   273     0.054484   1.197806   0.65      \n",
      "   274     0.049795   1.167656   0.65      \n",
      "   275     0.046543   1.175214   0.625     \n",
      "   276     0.043137   1.186379   0.625     \n",
      "   277     0.039723   1.217278   0.625     \n",
      "   278     0.068665   1.24294    0.65      \n",
      "   279     0.078324   1.278379   0.65      \n",
      "   280     0.07277    1.283747   0.65      \n",
      "   281     0.068282   1.283818   0.65      \n",
      "   282     0.067515   1.285344   0.65      \n",
      "   283     0.06217    1.314246   0.65      \n",
      "   284     0.072603   1.300829   0.65      \n",
      "   285     0.073979   1.324903   0.65      \n",
      "   286     0.068191   1.317367   0.625     \n",
      "   287     0.065805   1.265202   0.675     \n",
      "   288     0.061081   1.23401    0.625     \n",
      "   289     0.056623   1.22983    0.625     \n",
      "   290     0.073047   1.207519   0.65      \n",
      "   291     0.069479   1.155575   0.675     \n",
      "   292     0.067406   1.130747   0.7       \n",
      "   293     0.062582   1.131103   0.675     \n",
      "   294     0.058275   1.132185   0.675     \n",
      "   295     0.054628   1.144158   0.675     \n",
      "   296     0.050857   1.160026   0.675     \n",
      "   297     0.046788   1.181962   0.675     \n",
      "   298     0.047551   1.215909   0.675     \n",
      "   299     0.063597   1.211439   0.675     \n",
      "   300     0.060594   1.29811    0.675     \n",
      "   301     0.063476   1.348807   0.65      \n",
      "   302     0.058764   1.367964   0.65      \n",
      "   303     0.054337   1.35827    0.65      \n",
      "   304     0.060853   1.355234   0.65      \n",
      "   305     0.060851   1.330439   0.675     \n",
      "   306     0.057674   1.284504   0.675     \n",
      "   307     0.056414   1.254208   0.675     \n",
      "   308     0.058387   1.285936   0.675     \n",
      "   309     0.054767   1.279746   0.675     \n",
      "   310     0.055593   1.283333   0.675     \n",
      "   311     0.052076   1.266833   0.675     \n",
      "   312     0.050195   1.277825   0.65      \n",
      "   313     0.047797   1.392678   0.675     \n",
      "   314     0.046796   1.387919   0.65      \n",
      "   315     0.046669   1.440194   0.625     \n",
      "   316     0.045584   1.415459   0.625     \n",
      "   317     0.044976   1.403801   0.625     \n",
      "   318     0.044428   1.369654   0.65      \n",
      "   319     0.041898   1.362227   0.65      \n",
      "   320     0.039045   1.346975   0.65      \n",
      "   321     0.036579   1.29967    0.675     \n",
      "   322     0.058504   1.27405    0.65      \n",
      "   323     0.056419   1.292285   0.65      \n",
      "   324     0.067258   1.305007   0.65      \n",
      "   325     0.061985   1.286975   0.625     \n",
      "   326     0.057727   1.312955   0.625     \n",
      "   327     0.056516   1.308031   0.625     \n",
      "   328     0.054076   1.370781   0.625     \n",
      "   329     0.050013   1.335828   0.625     \n",
      "   330     0.046834   1.323749   0.625     \n",
      "   331     0.053281   1.287479   0.65      \n",
      "   332     0.049096   1.257487   0.65      \n",
      "   333     0.054523   1.264724   0.65      \n",
      "   334     0.080704   1.262644   0.65      \n",
      "   335     0.10553    1.29758    0.625     \n",
      "   336     0.098027   1.328365   0.625     \n",
      "   337     0.096592   1.354566   0.625     \n",
      "   338     0.095836   1.387422   0.575     \n",
      "   339     0.090439   1.331518   0.625     \n",
      "   340     0.083279   1.291172   0.625     \n",
      "   341     0.077342   1.261974   0.625     \n",
      "   342     0.071935   1.239918   0.65      \n",
      "   343     0.066272   1.263724   0.625     \n",
      "   344     0.061727   1.290254   0.675     \n",
      "   345     0.057555   1.278167   0.65      \n",
      "   346     0.052916   1.258619   0.65      \n",
      "   347     0.049491   1.282836   0.65      \n",
      "   348     0.049935   1.303645   0.675     \n",
      "   349     0.048077   1.368936   0.675     \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.36894]), 0.675000011920929]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch=resnet50\n",
    "data = ImageClassifierData.from_paths(PATH, tfms=tfms_from_model(arch, 224))\n",
    "learn = ConvLearner.pretrained(arch, data, precompute=True)\n",
    "learn.fit(0.0025, 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIC_0014541.jpg 0\n",
      "ISIC_0013242.jpg 1\n",
      "ISIC_0016043.jpg 0\n",
      "ISIC_0013374.jpg 1\n",
      "ISIC_0016072.jpg 0\n",
      "ISIC_0012425.jpg 0\n",
      "ISIC_0016053.jpg 1\n",
      "ISIC_0016071.jpg 1\n",
      "ISIC_0016046.jpg 1\n",
      "ISIC_0016064.jpg 0\n",
      "ISIC_0016036.jpg 1\n",
      "ISIC_0016044.jpg 0\n",
      "ISIC_0016057.jpg 1\n",
      "ISIC_0016022.jpg 1\n",
      "ISIC_0012356.jpg 0\n",
      "ISIC_0016035.jpg 1\n",
      "ISIC_0016049.jpg 1\n",
      "ISIC_0016045.jpg 1\n",
      "ISIC_0012758.jpg 0\n",
      "ISIC_0016031.jpg 1\n",
      "ISIC_0016063.jpg 1\n",
      "ISIC_0014513.jpg 0\n",
      "ISIC_0016068.jpg 1\n",
      "ISIC_0016024.jpg 1\n",
      "ISIC_0013073.jpg 1\n",
      "ISIC_0016041.jpg 0\n",
      "ISIC_0016027.jpg 0\n",
      "ISIC_0016042.jpg 0\n",
      "ISIC_0016034.jpg 0\n",
      "ISIC_0016028.jpg 0\n",
      "ISIC_0016054.jpg 0\n",
      "ISIC_0016069.jpg 1\n",
      "ISIC_0014963.jpg 0\n",
      "ISIC_0016038.jpg 1\n",
      "ISIC_0016051.jpg 0\n",
      "ISIC_0015251.jpg 0\n",
      "ISIC_0016056.jpg 0\n",
      "ISIC_0012369.jpg 1\n",
      "ISIC_0016048.jpg 1\n",
      "ISIC_0016030.jpg 0\n",
      "ISIC_0016029.jpg 1\n",
      "ISIC_0016070.jpg 0\n",
      "ISIC_0016055.jpg 0\n",
      "ISIC_0016065.jpg 0\n",
      "ISIC_0016059.jpg 1\n",
      "ISIC_0012092.jpg 0\n",
      "ISIC_0016052.jpg 1\n",
      "ISIC_0012395.jpg 1\n",
      "ISIC_0016061.jpg 0\n",
      "ISIC_0016060.jpg 1\n",
      "ISIC_0014772.jpg 0\n",
      "ISIC_0013277.jpg 0\n",
      "ISIC_0016026.jpg 0\n",
      "ISIC_0016019.jpg 0\n",
      "ISIC_0013321.jpg 0\n",
      "ISIC_0016062.jpg 1\n",
      "ISIC_0013411.jpg 0\n",
      "ISIC_0016037.jpg 1\n",
      "ISIC_0016033.jpg 1\n",
      "ISIC_0016058.jpg 0\n",
      "ISIC_0016023.jpg 0\n",
      "ISIC_0012989.jpg 0\n",
      "ISIC_0013072.jpg 1\n",
      "ISIC_0016025.jpg 0\n",
      "ISIC_0016040.jpg 0\n",
      "ISIC_0016066.jpg 1\n",
      "ISIC_0012258.jpg 0\n",
      "ISIC_0016018.jpg 0\n",
      "ISIC_0013414.jpg 0\n",
      "ISIC_0016050.jpg 1\n"
     ]
    }
   ],
   "source": [
    "# Test Images\n",
    "preds = []\n",
    "a = []\n",
    "for filename in os.listdir(f'{PATH}test/'):\n",
    "    if \".jpg\" not in filename:\n",
    "        continue\n",
    "    trn_tfms, val_tfms = tfms_from_model(arch,224) # get transformations\n",
    "    im = val_tfms(open_image(f'{PATH}test/{filename}'))\n",
    "    learn.precompute=False # We'll pass in a raw image, not activations\n",
    "    preds = abs(learn.predict_array(im[None]))\n",
    "#    if max(preds[0]) <= 6.00:\n",
    "        #print(filename,np.argmin(preds))\n",
    "    #else:\n",
    "     #   print(filename, np.argmax(preds))\n",
    "#     if min(preds[0]) == 0.0:\n",
    "#         print(filename, ' min too close to 0')\n",
    "    if min(preds[0]) == 0.0:\n",
    "        print(filename, '0')\n",
    "    elif max(preds[0])/min(preds[0]) < 100: \n",
    "        print(filename, '0')\n",
    "        #a.append(max(preds[0])/min(preds[0]))\n",
    "        pass\n",
    "    else:\n",
    "        print(filename, np.argmax(preds)) \n",
    "norm = [float(i)/sum(a) for i in a]\n",
    "t = [g for g in a if g <= 30.00]\n",
    "#print(t)    \n",
    "\n",
    "# Decent models: lr=0.0075, epochs=150, threshold=50\n",
    "# 15 20 30 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
